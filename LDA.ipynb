{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06a2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be1fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews and labels from the file\n",
    "with open('train_reviews.pkl', 'rb') as f:\n",
    "    train_reviews, train_labels = pickle.load(f)\n",
    "\n",
    "with open('test_reviews.pkl', 'rb') as f:\n",
    "    test_reviews, test_labels = pickle.load(f)\n",
    "    \n",
    "with open('unsup_reviews.pkl', 'rb') as f:\n",
    "    train_reviews_unsup = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e096b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer_model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all reviews for vectorization\n",
    "all_reviews = train_reviews + train_reviews_unsup\n",
    "\n",
    "# Convert the reviews to a document-term matrix\n",
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "X = vectorizer.fit_transform(all_reviews)\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac9e995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit an LDA model to learn word representations\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online', random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# Save the LDA model\n",
    "joblib.dump(lda, 'lda_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36c07fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# Get document-topic distributions from the LDA model\n",
    "doc_topic_distributions = lda.transform(X)\n",
    "\n",
    "# Function to incorporate sentiment annotations into word representations\n",
    "def incorporate_sentiment(doc_topic_distributions, labels, alpha=0.1):\n",
    "    n_docs, n_topics = doc_topic_distributions.shape\n",
    "    sentiment_topic_distributions = np.zeros((n_topics, 2))\n",
    "    \n",
    "    # Calculate the sentiment distribution for each topic\n",
    "    for topic in range(n_topics):\n",
    "        positive_sum = 0\n",
    "        negative_sum = 0\n",
    "        for i in range(n_docs):\n",
    "            if i < len(labels):  # Only use labeled data\n",
    "                if labels[i] == 1:\n",
    "                    positive_sum += doc_topic_distributions[i, topic]\n",
    "                else:\n",
    "                    negative_sum += doc_topic_distributions[i, topic]\n",
    "        sentiment_topic_distributions[topic, 0] = negative_sum\n",
    "        sentiment_topic_distributions[topic, 1] = positive_sum\n",
    "    \n",
    "    # Normalize the distributions\n",
    "    sentiment_topic_distributions = softmax(sentiment_topic_distributions, axis=1)\n",
    "    \n",
    "    # Adjust the topic-word distributions based on sentiment\n",
    "    topic_word_distributions = lda.components_\n",
    "    adjusted_topic_word_distributions = topic_word_distributions.copy()\n",
    "    for topic in range(n_topics):\n",
    "        for word in range(topic_word_distributions.shape[1]):\n",
    "            adjusted_topic_word_distributions[topic, word] += alpha * sentiment_topic_distributions[topic, 1]\n",
    "    \n",
    "    # Normalize the adjusted topic-word distributions\n",
    "    adjusted_topic_word_distributions /= adjusted_topic_word_distributions.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    return adjusted_topic_word_distributions\n",
    "\n",
    "# Incorporate sentiment information into word representations\n",
    "adjusted_topic_word_distributions = incorporate_sentiment(doc_topic_distributions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2088df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Use the document-topic distributions as features for sentiment classification\n",
    "X_train = doc_topic_distributions[:len(train_labels)]\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the logistic regression classifier\n",
    "joblib.dump(classifier, 'classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb08cf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.76\n",
      "Recall: 0.78\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19093ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review: bullshit\n",
      "The predicted sentiment for the review is: positive\n"
     ]
    }
   ],
   "source": [
    "# Function to predict sentiment of an input review\n",
    "def predict_sentiment(review, vectorizer, lda, classifier):\n",
    "    review_vector = vectorizer.transform([review])\n",
    "    review_topic_distribution = lda.transform(review_vector)\n",
    "    prediction = classifier.predict(review_topic_distribution)\n",
    "    sentiment = 'positive' if prediction[0] == 1 else 'negative'\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "input_review = input(\"Enter a review: \")\n",
    "predicted_sentiment = predict_sentiment(input_review, vectorizer, lda, classifier)\n",
    "print(f\"The predicted sentiment for the review is: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd64337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
