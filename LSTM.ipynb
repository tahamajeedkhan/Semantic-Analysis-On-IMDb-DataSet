{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e90674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5dbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews and labels from the file\n",
    "with open('train_reviews.pkl', 'rb') as f:\n",
    "    train_reviews, train_labels = pickle.load(f)\n",
    "\n",
    "with open('test_reviews.pkl', 'rb') as f:\n",
    "    test_reviews, test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3d4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized and padded sequences\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "\n",
    "# Load the one-hot encoded labels\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "max_words = 10000\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59701fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 480ms/step - accuracy: 0.6132 - loss: 0.6204\n",
      "Epoch 2/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 510ms/step - accuracy: 0.8921 - loss: 0.2712\n",
      "Epoch 3/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 518ms/step - accuracy: 0.9228 - loss: 0.2053\n",
      "Epoch 4/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 537ms/step - accuracy: 0.9418 - loss: 0.1611\n",
      "Epoch 5/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 562ms/step - accuracy: 0.9569 - loss: 0.1239\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model architecture\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "model.save('LSTM_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f27322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('LSTM_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ecea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 149ms/step\n",
      "Accuracy: 0.86\n",
      "Precision: 0.87\n",
      "Recall: 0.85\n",
      "F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = [1 if pred[1] > pred[0] else 0 for pred in y_pred]\n",
    "\n",
    "# Calculate and print accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(test_labels, y_pred_labels)\n",
    "precision = precision_score(test_labels, y_pred_labels)\n",
    "recall = recall_score(test_labels, y_pred_labels)\n",
    "f1 = f1_score(test_labels, y_pred_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af92f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a review: If you had asked me how the movie was throughout the film, I would have told you it was great! However, I left the theatre feeling unsatisfied. After thinking a little about it, I believe the problem was the pace of the ending. I feel that the majority of the movie moved kind of slow, and then the ending developed very fast. So, I would say the ending left me disappointed.<br /><br />I thought that the characters were well developed. Costner and Kutcher both portrayed their roles very well. Yes! Ashton Kutcher can act! Also, the different relationships between the characters seemed very real. Furthermore,I thought that the different plot lines were well developed. Overall, it was a good movie and I would recommend seeing it.<br /><br />In conclusion: Good Characters, Great Plot, Poorly Written/Edited Ending. Still, Go See It!!!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "The predicted sentiment for the review is: positive\n"
     ]
    }
   ],
   "source": [
    "# Take user input for the review\n",
    "input_review = input(\"Enter a review: \")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_review(review, tokenizer, maxlen):\n",
    "    # Convert the review to a sequence of integers\n",
    "    sequences = tokenizer.texts_to_sequences([review])\n",
    "    # Pad the sequences to ensure uniform input length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return padded_sequences\n",
    "\n",
    "# Preprocess the input review\n",
    "preprocessed_review = preprocess_review(input_review, tokenizer, max_review_length)\n",
    "\n",
    "# Predict the sentiment of the input review\n",
    "y_pred = model.predict(preprocessed_review)\n",
    "\n",
    "# Convert prediction to class label (1 for positive, 0 for negative)\n",
    "y_pred_label = 1 if y_pred[0][1] > y_pred[0][0] else 0\n",
    "\n",
    "# Print the predicted sentiment\n",
    "sentiment = \"positive\" if y_pred_label == 1 else \"negative\"\n",
    "print(f\"The predicted sentiment for the review is: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed71af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
